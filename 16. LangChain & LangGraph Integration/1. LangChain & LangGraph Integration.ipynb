{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  **LangChain & LangGraph Integration**\n",
    "\n",
    "| Topic                            | Purpose / Usage                                                                                                                        |\n",
    "| -------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Installation & Setup**         | `pip install streamlit langchain langgraph openai` (plus any connectors, e.g. `langchain-openai`) ([Streamlit Docs][1])                |\n",
    "| **Streamlit Chat UI**            | `st.chat_input()` + `st.chat_message()` to build interactive chatbot interfaces ([Streamlit Docs][2])                                  |\n",
    "| **Chains & Agents**              | Design LangChain chains or LangGraph agents triggered via Streamlit UI elements ([GitHub][3], [KDnuggets][4])                          |\n",
    "| **Conversation Memory**          | Use `StreamlitChatMessageHistory` or LangChain memory classes with session state to persist chat history ([LangChain][5], [Reddit][6]) |\n",
    "| **Callback Visualization**       | Use `StreamlitCallbackHandler` to render chain actions/LLM thoughts step-by-step                                                       |\n",
    "| **Chain Graph Rendering**        | Visualize LangGraph workflows using `st.graphviz_chart()` or pre-generated diagrams via `st.image()`                                   |\n",
    "| **Tool Use & RAG**               | Support file/document QA via upload â†’ embedding â†’ retrieval pipelines (e.g. Deepseek, Deepseek with audio, vector databases)           |\n",
    "| **Routing Logic with LangGraph** | Use LangGraph nodes to build conditional routing, multi-tool orchestration, triggered from UI events                                   |\n",
    "| **Debugging & Tracing Tools**    | Enable inspection using `st.expander()`, `st.json()`, or logs to examine agent decisions and traces                                    |\n",
    "| **Async / FastAPI Integration**  | For LangGraph workflows, route through FastAPI to manage streaming or async execution; UI then consumes via Streamlit app              |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ðŸ“„ 2. app.py â€“ Streamlit + LangChain Chatbot\n",
    "import streamlit as st\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.callbacks.streamlit import StreamlitCallbackHandler\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import os\n",
    "\n",
    "# --- PAGE CONFIG ---\n",
    "st.set_page_config(page_title=\"ðŸ§  LangChain Chatbot\", layout=\"centered\")\n",
    "st.title(\"ðŸ’¬ LangChain + Streamlit Chat\")\n",
    "\n",
    "# --- API KEY ---\n",
    "openai_key = st.secrets[\"OPENAI_API_KEY\"]\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "\n",
    "# --- SESSION STATE INIT ---\n",
    "if \"chat_history\" not in st.session_state:\n",
    "    st.session_state.chat_history = []\n",
    "\n",
    "# --- INPUT ---\n",
    "user_input = st.chat_input(\"Say something...\")\n",
    "\n",
    "# --- DISPLAY PAST MESSAGES ---\n",
    "for role, msg in st.session_state.chat_history:\n",
    "    with st.chat_message(role):\n",
    "        st.markdown(msg)\n",
    "\n",
    "# --- PROCESS INPUT ---\n",
    "if user_input:\n",
    "    # Show user message\n",
    "    st.session_state.chat_history.append((\"user\", user_input))\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(user_input)\n",
    "\n",
    "    # LangChain Memory\n",
    "    memory = ConversationBufferMemory(return_messages=True)\n",
    "\n",
    "    # Chat Model\n",
    "    llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "    # Prompt (optional â€“ shows how to inject your own)\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"history\", \"input\"],\n",
    "        template=\"You are a helpful assistant.\\nChat History:\\n{history}\\nUser: {input}\\nAssistant:\"\n",
    "    )\n",
    "\n",
    "    # Callback handler (see step-by-step trace)\n",
    "    callback = StreamlitCallbackHandler(parent_container=st.container())\n",
    "\n",
    "    # LangChain Chain\n",
    "    chain = ConversationChain(\n",
    "        llm=llm,\n",
    "        memory=memory,\n",
    "        prompt=prompt,\n",
    "        verbose=True,\n",
    "        callbacks=[callback],\n",
    "    )\n",
    "\n",
    "    # Get response\n",
    "    response = chain.run(input=user_input)\n",
    "\n",
    "    # Show assistant reply\n",
    "    st.session_state.chat_history.append((\"assistant\", response))\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        st.markdown(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ“„ 2. Define Your LangGraph Workflow â€“ graph_workflow.py\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Simple graph node: LLM replies with a basic answer\n",
    "def reply_node(state):\n",
    "    messages = state.get(\"messages\", [])\n",
    "    chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "    response = chat.invoke(messages)\n",
    "    messages.append(response)\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "# Define the LangGraph workflow\n",
    "def build_conversation_graph():\n",
    "    builder = StateGraph(schema={\"messages\": list})\n",
    "    \n",
    "    builder.add_node(\"respond\", RunnableLambda(reply_node))\n",
    "    builder.set_entry_point(\"respond\")\n",
    "    builder.set_finish_point(\"respond\")  # No loops for now\n",
    "\n",
    "    return builder.compile()\n",
    "\n",
    "\n",
    "\n",
    "# 3. Build Streamlit UI â€“ app.py\n",
    "\n",
    "import streamlit as st\n",
    "from langgraph.graph import MessageGraph\n",
    "from langchain_core.messages import HumanMessage\n",
    "from graph_workflow import build_conversation_graph\n",
    "\n",
    "# Page setup\n",
    "st.set_page_config(page_title=\"LangGraph Chat\", layout=\"centered\")\n",
    "st.title(\"ðŸ¤– LangGraph + Streamlit\")\n",
    "\n",
    "# Initialize state\n",
    "if \"history\" not in st.session_state:\n",
    "    st.session_state.history = []\n",
    "    st.session_state.graph = build_conversation_graph()\n",
    "    st.session_state.runner = st.session_state.graph.invoke\n",
    "\n",
    "# User input\n",
    "user_input = st.chat_input(\"Type your message\")\n",
    "\n",
    "# Display past messages\n",
    "for msg in st.session_state.history:\n",
    "    with st.chat_message(msg[\"role\"]):\n",
    "        st.markdown(msg[\"content\"])\n",
    "\n",
    "# Handle new message\n",
    "if user_input:\n",
    "    # Show user message\n",
    "    st.session_state.history.append({\"role\": \"user\", \"content\": user_input})\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(user_input)\n",
    "\n",
    "    # Create message list for LangGraph\n",
    "    messages = [HumanMessage(content=user_input)]\n",
    "\n",
    "    # Run the LangGraph workflow\n",
    "    result = st.session_state.runner({\"messages\": messages})\n",
    "    response_msg = result[\"messages\"][-1]\n",
    "\n",
    "    # Show assistant response\n",
    "    st.session_state.history.append({\"role\": \"assistant\", \"content\": response_msg.content})\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        st.markdown(response_msg.content)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai-doc-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
